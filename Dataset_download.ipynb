{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a63b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download proteins and other features from InterPro for a given pfam protein family\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3551a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PF16754, PF06737, PF01832, PF05838, PF00959\n",
    "query = \"PF16754\"\n",
    "api_url = \"https://www.ebi.ac.uk/interpro/api\"\n",
    "url = f\"{api_url}/protein/uniprot/entry/pfam/{query}/\"\n",
    "url += \"?page_size=20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a0a0eww5&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a109w3y5&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a1e7yk54&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a1j6xb34&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a1u9uqb7&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a2a5a7s6&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a2h9t0a7&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a2r7ngb8&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a344teh3&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a388sm55&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a3p3qnh9&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a431xa18&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a4y1nu07&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a553laq6&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a5x0zgg2&page_size=20\n",
      "chunk arrived\n",
      "https://www.ebi.ac.uk/interpro/api/protein/uniprot/entry/pfam/PF16754/?cursor=source%3As%3Aa0a6h9tk68&page_size=20\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "next_url = url\n",
    "\n",
    "max_retries = 20\n",
    "retry_delay = 3  # Delay in seconds between retries\n",
    "\n",
    "retry_count = 0\n",
    "while next_url and retry_count < max_retries:\n",
    "    try:\n",
    "        with urlopen(next_url) as response:\n",
    "            result = json.loads(response.read().decode(\"utf-8\"))\n",
    "            data.extend(result[\"results\"])\n",
    "            next_url = result.get(\"next\")\n",
    "            retry_count = 0  # Reset retry count on successful request\n",
    "            print(\"chunk arrived\")\n",
    "            print(next_url)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        retry_count += 1\n",
    "        sleep(retry_delay)  # Delay before retrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54973db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the downloaded data\n",
    "max_retries = 8\n",
    "retry_delay = 2  # Delay in seconds between retries\n",
    "retry_count = 0\n",
    "total_len = 0\n",
    "total_sequences = 0\n",
    "filtered_data = []\n",
    "\n",
    "for result in data:\n",
    "    metadata = result[\"metadata\"]\n",
    "    accession = metadata[\"accession\"]\n",
    "    name = metadata[\"name\"]\n",
    "    source_database = metadata[\"source_database\"]\n",
    "    length = metadata[\"length\"]\n",
    "    source_organism = metadata[\"source_organism\"][\"scientificName\"]\n",
    "\n",
    "    sequence_url = f\"{api_url}/protein/uniprot/{accession}\"\n",
    "    \n",
    "    sequence = \"N/A\"  # Default value for sequence\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            with urlopen(sequence_url) as response:\n",
    "                sequence = json.loads(response.read().decode(\"utf-8\"))\n",
    "            break  # Break out of retry loop on successful request\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred while retrieving the sequence:\", str(e))\n",
    "            retry_count += 1\n",
    "            sleep(retry_delay)  # Delay before retrying\n",
    "    if sequence != \"N/A\":\n",
    "        result[\"sequence\"] = sequence\n",
    "        total_len += length\n",
    "        total_sequences += 1\n",
    "        filtered_data.append(result)\n",
    "        #print(\"Accession:\", accession)\n",
    "        #print(\"Name:\", name)\n",
    "        #print(\"Source Database:\", source_database)\n",
    "        #print(\"Length:\", length)\n",
    "        #print(\"Source Organism:\", source_organism)\n",
    "        #print(\"Sequence:\", sequence)\n",
    "        #print(\"---\")\n",
    "print(\"Sequences retrieved: \", total_sequences)\n",
    "print(\"Sequence average length: \", total_len/total_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Calculate the number of entries to select (10% of the data)\n",
    "# ID_test_percentage = 0.1\n",
    "# num_entries_to_select = int(len(filtered_data) * ID_test_percentage)\n",
    "\n",
    "# Randomly select ID_test_percentage of the data entries\n",
    "# random_selection = random.sample(filtered_data, num_entries_to_select)\n",
    "\n",
    "# Print the randomly selected entries\n",
    "#for entry in random_selection:\n",
    "#    print(entry)\n",
    "#    break\n",
    "\n",
    "# Remove the randomly selected entries from `filtered_data` if needed\n",
    "# filtered_data = [entry for entry in filtered_data if entry not in random_selection]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create a directory named \"data\" in the current working directory if it doesn't exist\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Save filtered_data as a .p file\n",
    "filtered_data_file = os.path.join(data_dir, \"filtered_data_\" + query + \".p\")\n",
    "with open(filtered_data_file, \"wb\") as file:\n",
    "    pickle.dump(filtered_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save random_selection of 10 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 10)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_10_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 100 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 100)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_100_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 500 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 500)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_500_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 1000 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 1000)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_1000_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 2000 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 2000)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_2000_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 5000 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 5000)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_5000_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)\n",
    "# Save random_selection of 10000 proteins as a .p file\n",
    "random_selection = random.sample(filtered_data, 10000)\n",
    "random_selection_file = os.path.join(data_dir, \"random_selection_10000_\" + query + \".p\")\n",
    "with open(random_selection_file, \"wb\") as file:\n",
    "    pickle.dump(random_selection, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09818f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
