{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4684ca27-c854-490c-ab1b-900e907f3264",
   "metadata": {},
   "source": [
    "<h1>Finenzyme generation and fine-tuning tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bca07a-bed1-460b-9ab1-e00645a7286a",
   "metadata": {},
   "source": [
    "The generation process through the Protein Language Model Finenzyme involves several steps:\n",
    "\n",
    "1. **Tokenization**: The input text composed of conditioning tags and an amino acid sequence is translated into tokens.\n",
    "\n",
    "2. **Feeding the Model**: The tokens are then fed into the model. The model takes in a sequence of tokens and returns a probability distribution for the next token - that is, it assigns a probability to every possible next token over the amino acid vocabulary.\n",
    "\n",
    "3. **Sampling**: A token is selected from this distribution. The way this selection happens can vary - it could be the token with the highest probability, or it could be a random selection weighted by the probabilities.\n",
    "\n",
    "4. **Decoding**: The selected token is then added to the sequence of input tokens, and the process repeats: the extended sequence is fed back into the model, which returns a new probability distribution for the next token, and so on. This continues until a stop condition is met, such as reaching a maximum length or generating a special end-of-sequence stop token.\n",
    "\n",
    "5. **Detokenization**: Finally, the sequence of tokens is converted back into an amino acid sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f76eb-5e79-4b04-9275-0fc1a369952e",
   "metadata": {},
   "source": [
    "<h2>Step 1. Generation through the pre-trained model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848b992-0143-4def-8562-47a7d8492c76",
   "metadata": {},
   "source": [
    "In this section, we will generate sequences using the pre-trained Finenzyme model. This involves several steps, including specifying UniProt keywords, writing the code to perform the generation, and using taxonomy keywords\n",
    "\n",
    "#### Process Overview\n",
    "1. **Model Loading**: First, we load a pre-trained model that has been trained on a large dataset of protein sequences. This model is capable of generating new sequences based on the patterns it has learned.\n",
    "2. **Sequence Generation Example**: Using the model, we generate a new protein sequence. This can be done by providing a starting sequence or keywords that guide the generation process (in the example below, we use both).\n",
    "3. **Keywords Specification**: We specify keywords from the UniProt database. These keywords help in filtering and refining the sequences based on biological functions, and structures.\n",
    "4. **Taxonomy Keyword**: Additionally, we use a taxonomy keyword to focus the generation on a specific organism.\n",
    "\n",
    "#### UniProt Keywords\n",
    "UniProt keywords are terms used to describe various aspects of protein sequences, including their functions, domains, and cellular locations. By specifying these keywords, we can guide the model to generate sequences that have desired properties. Examples of UniProt keywords include:\n",
    "- **Kinase**: To generate sequences related to kinases.\n",
    "- **Membrane**: For sequences associated with membrane proteins.\n",
    "- **DNA-binding**: To focus on proteins that bind to DNA.\n",
    "\n",
    "#### Taxonomy Keywords\n",
    "Taxonomy keywords allow us to filter sequences based on the organism or group of organisms of interest. By incorporating these keywords, we can ensure the generated sequences are relevant to specific taxonomic groups. Examples of taxonomy keywords include:\n",
    "- **Homo sapiens**: For human-related sequences.\n",
    "- **Escherichia coli**: To focus on sequences from E. coli.\n",
    "- **Fungi**: For sequences related to fungal organisms.\n",
    "\n",
    "By combining UniProt and taxonomy keywords, we can guide the generation process of protein sequences using the pre-trained model.\n",
    "\n",
    "#### Code\n",
    "The code for generating sequences through the pre-trained model involves loading the model, specifying the generation parameters, and executing the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d934c1d8-b018-4aa9-a529-80ad6ab026a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_manager import GeneratorManager\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc03b52-72e8-41a7-b869-dd5978b2d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SIZE: \n",
      "1280\n",
      "Found PyTorch checkpoint at  ckpt/pretrain_progen_full.pth\n",
      "GPU aviable. Previous checkpoint loaded in GPU\n"
     ]
    }
   ],
   "source": [
    "# Here we set the model checkpoint path, \n",
    "# for this example, we use Finenzyme pre-trained model.\n",
    "model_path = 'ckpt/pretrain_progen_full.pth' \n",
    "\n",
    "# Now, time to set the generator manager with default parameters: penalty = 0, and let's set top-k sampling, with k = 1\n",
    "# This class loads the model and in memory.\n",
    "generator = GeneratorManager(model_path, topk = 1)\n",
    "\n",
    "# Now, let's load the tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b45e5c-9a82-46f4-845e-72e34a53d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from uniProt, translated into Finenzyme control codes:  [13, 29, 422, 49, 3]\n",
      "Do we have the taxonomy ID in the tokenizer?  True\n",
      "taxonomy id:  1634\n"
     ]
    }
   ],
   "source": [
    "# An example sequence: P05102\tMTH1_HAEPH\tType II methyltransferase M.HhaI \n",
    "# (source: https://www.uniprot.org/uniprotkb/P05102/entry)\n",
    "sequence = \"MIEIKDKQLTGLRFIDLFAGLGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\"\n",
    "\n",
    "# keyword IDs in input (from UniProt): \n",
    "keywords_uniprot = 'KW-0002; KW-0238; KW-0489; KW-0680; KW-0949; KW-0808'\n",
    "keywords_uniprot = [int(i.split('-')[-1]) for i in keywords_uniprot.split('; ') if i != '']\n",
    "\n",
    "# Convert keywords to control IDs, \n",
    "# given that UniProt goes under continuous updates, we might not have all keywords in the encoding dictionary\n",
    "keywords_finenzyme = [tokenizer.kw_to_ctrl_idx[i] for i in keywords_uniprot if i in tokenizer.kw_to_ctrl_idx.keys()]\n",
    "print('Keywords from uniProt, translated into Finenzyme control codes: ', keywords_finenzyme)\n",
    "\n",
    "# we can also add taxonomy ID as a keyword\n",
    "taxonomy_id = 735 # from UniProt\n",
    "taxonomy_id = tokenizer.taxa_to_ctrl_idx[taxonomy_id] if taxonomy_id in tokenizer.taxa_to_ctrl_idx.keys() else None\n",
    "print('Do we have the taxonomy ID in the tokenizer? ', taxonomy_id is not None)\n",
    "print('taxonomy id: ', taxonomy_id)\n",
    "\n",
    "# next, we add the taxonomy Finenzyme tag into the list of tags in input to the model\n",
    "keywords_finenzyme.append(taxonomy_id)\n",
    "\n",
    "# Next, we set the amino acid prefix to give in input to the model\n",
    "prefix = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "588115bb-787a-4eca-8c79-9b3331e0a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: generation. With after_n_generation we generate up to the real protein length.\n",
    "res, tokens_prob, offset = generator.after_n_generation(sequence, keywords_finenzyme, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f007c84-db45-4c8e-85e0-845e16463a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGen generated a protein with a prefix of  20 amino acids.\n",
      "Is the predicted protein equal to the real one? False\n",
      "Generated protein:\n",
      "IGGFHAALHRLGGRCVYASEIDPHVRKVYELNFGDRPEFTEDIRKITEEEIPDHDVLLAGFPCQPFSIIGKRRGFKDERGTLYFEILRILKAKRPRAFLLENVKNFVNHDKGRTFKIIEDVLEELDFSFSYKLLDPKNFGVPQNRERVFIVGFREKEKLDFKFPKPEELPKPLTLSDILEDNPDSQYFLSKDKLTKLHRHKEKGNGFGFGLVNIEGKGGKIARTLSARYHKDNIDNIEGARNNARPNHQANGIPLSPQQAAKIQGFPEDFKIIGNDAVYKQLGNAVVVPLIQAIGEKILKELNKEKK\n",
      "Actual protein:\n",
      "LGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\n"
     ]
    }
   ],
   "source": [
    "# What happened during generation?\n",
    "print('Finenzyme generated a protein with a prefix of ', offset, 'amino acids.')\n",
    "print('Is the predicted protein equal to the real one?', res == sequence[prefix:])\n",
    "print('Generated protein:')\n",
    "print(res)\n",
    "print('Actual protein:')\n",
    "print(sequence[prefix:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57794df1-2d32-4113-af53-9f1cb9f7b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained model generated an enzyme with 259 different amino acids from the original one.\n"
     ]
    }
   ],
   "source": [
    "# here we search for indices of amino acids that differ from the natural sequence\n",
    "differing_idx = [(i, [actual, predicted]) for i, (actual, predicted) in enumerate(zip(sequence[prefix:], res)) if actual != predicted]\n",
    "print(f'The pre-trained model generated an enzyme with {len(differing_idx)} different amino acids from the original one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b25b64b6-0904-49b0-899a-ff0eb3df271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sequence in generated index 0, has probabilities for true aa L of 0.169, and for predicted aa I of 0.666\n"
     ]
    }
   ],
   "source": [
    "# we can analyze the probabilities of these indices that we have in output from Finenzyme\n",
    "from tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "for difference in differing_idx:\n",
    "    print(f'The predicted sequence in generated index {difference[0]},'\n",
    "    f' has probabilities for true aa {difference[1][0]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][0]]]:.3f},'\n",
    "    f' and for predicted aa {difference[1][1]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][1]]]:.3f}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f400fbd-f071-40e0-9238-8c8b34ee1e5a",
   "metadata": {},
   "source": [
    "### Comment on the Generation with the Pre-trained Model\n",
    "\n",
    "In this section, we discuss the results obtained from generating protein sequences using the pre-trained model. The model was guided by specific UniProt keywords and a taxonomy ID to produce sequences relevant to the desired biological context.\n",
    "\n",
    "#### Results Analysis\n",
    "The generation process began with the input sequence of the Type II methyltransferase P05102 and relevant UniProt keywords related to this entry, translated into control codes for the model. A taxonomy ID was also added to refine the context further.\n",
    "\n",
    "The model generated a sequence with a specified prefix length, and the resulting sequence was compared to the original protein sequence.\n",
    "\n",
    "- **Generated Sequence**: The pre-trained model produced a protein sequence with an initial prefix of 20 amino acids provided as input. The rest of the sequence was generated based on the model's learned patterns and the specified keywords.\n",
    "- **Comparison to Actual Sequence**: The generated sequence was compared to the actual sequence starting from the 20th amino acid. It was noted that there were differences in several amino acids between the generated and actual sequences.\n",
    "- **Differing Indices**: The specific indices where the generated sequence differed from the actual sequence were identified. The analysis found a certain number of amino acids that did not match, indicating areas where the model's predictions deviated from the natural sequence.\n",
    "- **Probability Analysis**: For the differing amino acids, the probabilities assigned by the model to both the actual and predicted amino acids were examined. This provides insight into the model's confidence in its predictions and highlights the areas where it may have uncertain.\n",
    "\n",
    "#### Conclusion\n",
    "The pre-trained model generated a protein sequence influenced by the input keywords and taxonomy ID. Even with the use of strict sampling methods (top-k with k = 1), the generated sequence was not identical to the natural one, the differences provide valuable information about the model's predictive capabilities for further improvement (i.e. fine-tuning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cfa88-005a-465f-b639-b033a214286d",
   "metadata": {},
   "source": [
    "<h2>Step 2. Fine-tuning the model on a specific protein set</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d6f61-e61c-4b17-bcb3-21cb1817b4e4",
   "metadata": {},
   "source": [
    "Fine-tuning is a critical step to adapt the pre-trained model to a specific set of data or a particular task. In this section, we describe the process of fine-tuning the model, starting from the dataset preparation to the actual fine-tuning process.\n",
    "\n",
    "#### Dataset Preparation\n",
    "To fine-tune the model, we need a dataset that is formatted correctly for the training process. This dataset should be created by following the `load_tsv_uniprot_dataset` notebook. The notebook takes as input a dataset downloaded from UniProt in TSV (Tab-Separated Values) format and converts it into the format required by the model.\n",
    "\n",
    "1. **Loading the Dataset**: The dataset downloaded from UniProt contains protein sequences and associated metadata. This raw data needs to be processed to fit the model's requirements.\n",
    "2. **Defining Keywords**: During the conversion process, we define the fine-tuning keywords and their meanings. These keywords are essential for guiding the model during the fine-tuning phase. Examples of keywords might include specific protein functions, domains, or cellular locations relevant to the task.\n",
    "3. **Encoding Control Codes**: The defined keywords are then encoded into control codes. These control codes are integrated directly into the dataset, allowing the model to recognize and utilize them during training.\n",
    "\n",
    "    #### Fine-tuned Model Vocabulary\n",
    "    Assumptions:\n",
    "    - There are \\( k \\) clusters replacing control codes [0, \\( k-1 \\)]\n",
    "    - There is a stop token replacing control code \\( k \\)\n",
    "    - The sample length is 511. All extra tokens are replaced with the original pad token.\n",
    "\n",
    "    For instance, for fine-tuning the model on EC 2.1.1.37, we can define the phage keyword as '0', and the stop token as '1'.\n",
    "\n",
    "4. **Protein Sequences**: While the keywords are encoded, the actual protein sequences are left unchanged. Their encoding is handled automatically during the training step.\n",
    "5. **Dataset Split**: The dataset can be splitted into training, test, and validation sets to ensure the model is properly evaluated and tuned (more details and implementation in the `load_tsv_uniprot_dataset` notebook).\n",
    "\n",
    "#### Fine-tuning Process\n",
    "Once the dataset is prepared and formatted correctly, the fine-tuning process can begin. This involves training the model on the new dataset with the encoded keywords, allowing the model to learn and adapt to the specific characteristics and requirements of the task at hand.\n",
    "\n",
    "During fine-tuning:\n",
    "- The model adjusts its parameters based on the new data, enhancing its ability to generate or predict sequences that align with the encoded keywords and control codes.\n",
    "- The training process is guided by the defined keywords, ensuring that the model focuses on the relevant aspects of the protein sequences.\n",
    "\n",
    "The fine-tuning process is crucial for adapting the pre-trained model to specific tasks, improving its performance, and ensuring that it produces biologically relevant and accurate results.\n",
    "\n",
    "#### Code\n",
    "The fine-tuning of Finenzyme is performed using the `pytorch_training.py` module. This can be executed directly from the command line by providing the pre-trained model, the training dataset, optionally the validation dataset, and all the fine-tuning parameters.\n",
    "\n",
    "**DISCLAIMER**: The fine-tuning step requires a significant amount of time, and parallelized hardware (GPU) is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acb223-7f97-4352-90f6-662d69df99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command as a string\n",
    "command = \"\"\"\n",
    "python pytorch_training.py --model_dir 'ckpt/' --model_path 'ckpt/pretrain_progen_full.pth' --stop_token 1 --model_name 'ec_2_1_1_37' --db_directory 'data_specific_enzymes/databases/pickles/'\n",
    "\"\"\"\n",
    "\n",
    "# Use subprocess to run the command\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Print the output line by line as it's being generated\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    print(line.decode(), end='')\n",
    "\n",
    "# Print the error messages, if any\n",
    "for line in iter(process.stderr.readline, b''):\n",
    "    print(line.decode(), end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e95b5-0b55-418b-b6ea-0bc747f2433c",
   "metadata": {},
   "source": [
    "### Fine-tuning Parameters\n",
    "The fine-tuning process for the model involves several key parameters, managing the model's learning process. Here is a detailed description of the most important parameters used in the fine-tuning process:\n",
    "\n",
    "1. **model_dir**: This parameter specifies the directory where the training model checkpoints will be saved. By default, it is set to `'ckpt/'`.\n",
    "\n",
    "2. **model_path**: This parameter indicates the path to the model checkpoint file to load. It is the specific file within the model directory that contains the pre-trained model's weights. The default value is `'ckpt/pretrain_progen_full.pth'`.\n",
    "\n",
    "3. **sequence_len**: This parameter defines the sequence length of the model being fine-tuned. It ensures that all sequences are the same length for training. The default length is `511`.\n",
    "\n",
    "4. **num_epochs**: This parameter specifies the number of epochs for which the model will be trained. An epoch is a full pass through the training dataset. The default value is `8`.\n",
    "\n",
    "5. **batch_size**: This parameter determines the number of samples processed before the model is updated. The default batch size is `2`.\n",
    "\n",
    "6. **warmup_iteration**: This parameter indicates the number of iterations for learning rate warm-up. During this period, the learning rate gradually increases to its initial value. The default is `1000`.\n",
    "\n",
    "7. **save_iter**: This parameter specifies how often (in terms of iterations) the model checkpoint should be saved. The default value is every `10` iterations.\n",
    "\n",
    "8. **stop_token**: This parameter defines the stop token for fine-tuning, which signals the end of a sequence. The stop token value depends on the fine-tuning vocabulary defined during the dataset creation.\n",
    "\n",
    "9. **model_name**: This parameter sets the name of the fine-tuning model. The default is `'fine_tuned_model_test'`.\n",
    "\n",
    "10. **db_directory**: This parameter points to the directory where the fine-tuning dataset is stored. The default path is `'data_specific_enzymes/databases/pickles/'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd404ea-393f-4f85-866e-2973a4788765",
   "metadata": {},
   "source": [
    "<h2>Step 3. Generation through the fine-tuned model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76061741-2cd8-4a99-8921-78333e23b551",
   "metadata": {},
   "source": [
    "The generation process using the fine-tuned model (Finenzyme) is designed to produce protein sequences that are more relevant and specific to the tasks or characteristics defined during the fine-tuning phase. The fine-tuned model is utilized after it has been trained on a dataset tailored to the desired outputs, incorporating control codes and keywords to guide the generation.\n",
    "\n",
    "**Steps Involved:**\n",
    "1. **Loading the Model**: Finenzyme is loaded from a checkpoint that stores the trained state of the model. This ensures that all the adaptations made during the fine-tuning process are retained.\n",
    "   \n",
    "2. **Generating Sequences**: Finenzyme, similarly to the pre-trained one can generate sequences by taking an initial sequence or context and extending it based on the patterns it learned during fine-tuning. The generation process is influenced by the specific keywords and control codes defined in the fine-tuning dataset.\n",
    "\n",
    "3. **Keywords Specification**: During generation with Finenzyme, it is possible to use the keywords defined duting fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b3f274-7635-4e28-974c-963116e70fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_manager import GeneratorManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6828d655-0275-414a-8b7d-540e42c26462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SIZE: \n",
      "1280\n",
      "Found PyTorch checkpoint at  ckpt/ec_2_1_1_37_warmup_1000_earlystop_epoch6_015_flip_LR01_2batch.pth\n",
      "GPU aviable. Previous checkpoint loaded in GPU\n"
     ]
    }
   ],
   "source": [
    "# Here we set the model checkpoint path, \n",
    "# for this example, we use Finenzyme on phage lysozymes.\n",
    "model_path = 'ckpt/ec_2_1_1_37_warmup_1000_earlystop_epoch6_015_flip_LR01_2batch.pth' \n",
    "\n",
    "# Now, time to set the generator manager with default parameters: penalty = 0, and let's set top-k = 1\n",
    "# This class loads the model and the tokenizer in memory.\n",
    "generator = GeneratorManager(model_path, topk = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26faedec-f4d1-471f-a617-66837d67837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example sequence: P05102\tMTH1_HAEPH\tType II methyltransferase M.HhaI \n",
    "# (source: https://www.uniprot.org/uniprotkb/P05102/entry)\n",
    "sequence = \"MIEIKDKQLTGLRFIDLFAGLGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\"\n",
    "\n",
    "# keyword IDs in input defined during fine-tuning: (already in control code codification) \n",
    "keywords_finenzyme = [0]\n",
    "\n",
    "# Next, we set the amino acid prefix to give in input to the model\n",
    "prefix = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68418986-0abe-48a7-b1f4-648709cff8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: generation. With after_n_generation we generate up to the real protein length.\n",
    "res, tokens_prob, offset = generator.after_n_generation(sequence, keywords_finenzyme, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed03951d-9541-41e5-a1e5-7f7681ea86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGen generated a protein with a prefix of  20 amino acids.\n",
      "Is the predicted protein equal to the real one? False\n",
      "Generated protein:\n",
      "LGGFRLALESFGAECVYSNEWDKYAQEVYQMNFGDKPDGDITLVDENSVPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDVARIVKAKNPKVVFMENVKNFASHDNGNTLKVVKNIMVDLGYDFYSDVLNSLDFGIPQKRERIYMVCFRKDLNIKNFTFPKPFKLSTFLEDLLLPDEEVSNLIINRPDLVLKDIEIKNNSNKTIRIGEVGKGGQGERIYSPKGIAITLSAYGGGVFSKTGGYLINGKTRKLHPRECARIMGYPDSYLIHPSWNQAYKQFGNSVVVNVLQYITKNMGEALSGEYN\n",
      "Actual protein:\n",
      "LGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\n"
     ]
    }
   ],
   "source": [
    "# What happened during generation?\n",
    "print('Finenzyme generated a protein with a prefix of ', offset, 'amino acids.')\n",
    "print('Is the predicted protein equal to the real one?', res == sequence[prefix:])\n",
    "print('Generated protein:')\n",
    "print(res)\n",
    "print('Actual protein:')\n",
    "print(sequence[prefix:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd134435-be0b-4921-9bb1-fc99a236da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained model generated an enzyme with 71 different amino acids from the original one.\n"
     ]
    }
   ],
   "source": [
    "# here we search for indices of amino acids that differ from the natural sequence\n",
    "differing_idx = [(i, [actual, predicted]) for i, (actual, predicted) in enumerate(zip(sequence[prefix:], res)) if actual != predicted]\n",
    "print(f'The pre-trained model generated an enzyme with {len(differing_idx)} different amino acids from the original one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36d019cb-3ddb-44c2-9831-16fd0bd60279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sequence in index 10, has probabilities for true aa C of 0.017, and for predicted aa F of 0.796\n"
     ]
    }
   ],
   "source": [
    "# we can analyze the probabilities of these indices that we have in output from Finenzyme\n",
    "from tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "for difference in differing_idx:\n",
    "    print(f'The predicted sequence in index {difference[0]},'\n",
    "    f' has probabilities for true aa {difference[1][0]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][0]]]:.3f},'\n",
    "    f' and for predicted aa {difference[1][1]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][1]]]:.3f}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db7dc-1af7-4e62-8804-8b4fcc526eff",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "The generation process (following the pre-trained generation process in this notebook) began with the input sequence of the Type II methyltransferase P05102 and the keyword defined during fine-tuning related to the EC family of this entry, namely EC 2.1.1.37.\n",
    "\n",
    "The model generated a sequence with a specified prefix length, and the resulting sequence was compared to the original protein sequence.\n",
    "\n",
    "- **Generated Sequence**: The pre-trained model produced a protein sequence with an initial prefix of 20 amino acids provided as input. The rest of the sequence was generated based on the model's learned patterns and the specified keywords.\n",
    "- **Comparison to Actual Sequence**: The generated sequence was compared to the actual sequence starting from the 20th amino acid. It was noted that there were differences in several amino acids between the generated and actual sequences.\n",
    "- **Differing Indices**: The specific indices where the generated sequence differed from the actual sequence were identified. The analysis found a certain number of amino acids that did not match, indicating areas where the model's predictions deviated from the natural sequence.\n",
    "- **Probability Analysis**: For the differing amino acids, the probabilities assigned by the model to both the actual and predicted amino acids were examined. This provides insight into the model's confidence in its predictions and highlights the areas where it may have uncertain.\n",
    "\n",
    "#### Conclusion\n",
    "Finenzyme generated a protein sequence influenced by the input fine-tuned keyword. Using the same sampling methods (top-k with k = 1) as before with the pre-trained model, the generated sequence was still not identical to the natural one, but closer: 71 different amino acids instead of 259."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
