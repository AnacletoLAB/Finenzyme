{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4684ca27-c854-490c-ab1b-900e907f3264",
   "metadata": {},
   "source": [
    "<h1>Finenzyme generation and fine-tuning tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bca07a-bed1-460b-9ab1-e00645a7286a",
   "metadata": {},
   "source": [
    "The generation process through the Protein Language Model Finenzyme involves several steps:\n",
    "\n",
    "1. **Tokenization**: The input text composed of conditioning tags and an amino acid sequence is translated into tokens.\n",
    "\n",
    "2. **Feeding the Model**: The tokens are then fed into the model. The model takes in a sequence of tokens and returns a probability distribution for the next token - that is, it assigns a probability to every possible next token over the amino acid vocabulary.\n",
    "\n",
    "3. **Sampling**: A token is selected from this distribution. The way this selection happens can vary - it could be the token with the highest probability, or it could be a random selection weighted by the probabilities.\n",
    "\n",
    "4. **Decoding**: The selected token is then added to the sequence of input tokens, and the process repeats: the extended sequence is fed back into the model, which returns a new probability distribution for the next token, and so on. This continues until a stop condition is met, such as reaching a maximum length or generating a special end-of-sequence stop token.\n",
    "\n",
    "5. **Detokenization**: Finally, the sequence of tokens is converted back into an amino acid sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f76eb-5e79-4b04-9275-0fc1a369952e",
   "metadata": {},
   "source": [
    "<h2>Step 1. Generation through the pre-trained model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848b992-0143-4def-8562-47a7d8492c76",
   "metadata": {},
   "source": [
    "In this section, we will generate sequences using the pre-trained Finenzyme model. This involves several steps, including specifying UniProt keywords, writing the code to perform the generation, and using taxonomy keywords\n",
    "\n",
    "#### Process Overview\n",
    "1. **Model Loading**: First, we load a pre-trained model that has been trained on a large dataset of protein sequences. This model is capable of generating new sequences based on the patterns it has learned.\n",
    "2. **Sequence Generation Example**: Using the model, we generate a new protein sequence. This can be done by providing a starting sequence or keywords that guide the generation process (in the example below, we use both).\n",
    "3. **Keywords Specification**: We specify keywords from the UniProt database. These keywords help in filtering and refining the sequences based on biological functions, and structures.\n",
    "4. **Taxonomy Keyword**: Additionally, we use a taxonomy keyword to focus the generation on a specific organism.\n",
    "\n",
    "#### UniProt Keywords\n",
    "UniProt keywords are terms used to describe various aspects of protein sequences, including their functions, domains, and cellular locations. By specifying these keywords, we can guide the model to generate sequences that have desired properties. Examples of UniProt keywords include:\n",
    "- **Kinase**: To generate sequences related to kinases.\n",
    "- **Membrane**: For sequences associated with membrane proteins.\n",
    "- **DNA-binding**: To focus on proteins that bind to DNA.\n",
    "\n",
    "#### Taxonomy Keywords\n",
    "Taxonomy keywords allow us to filter sequences based on the organism or group of organisms of interest. By incorporating these keywords, we can ensure the generated sequences are relevant to specific taxonomic groups. Examples of taxonomy keywords include:\n",
    "- **Homo sapiens**: For human-related sequences.\n",
    "- **Escherichia coli**: To focus on sequences from E. coli.\n",
    "- **Fungi**: For sequences related to fungal organisms.\n",
    "\n",
    "By combining UniProt and taxonomy keywords, we can guide the generation process of protein sequences using the pre-trained model.\n",
    "\n",
    "#### Code\n",
    "The code for generating sequences through the pre-trained model involves loading the model, specifying the generation parameters, and executing the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d934c1d8-b018-4aa9-a529-80ad6ab026a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_manager import GeneratorManager\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc03b52-72e8-41a7-b869-dd5978b2d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SIZE: \n",
      "1280\n",
      "Found PyTorch checkpoint at  ckpt/pretrain_progen_full.pth\n",
      "GPU aviable. Previous checkpoint loaded in GPU\n"
     ]
    }
   ],
   "source": [
    "# Here we set the model checkpoint path, \n",
    "# for this example, we use Finenzyme pre-trained model.\n",
    "model_path = 'ckpt/pretrain_progen_full.pth' \n",
    "\n",
    "# Now, time to set the generator manager with default parameters: penalty = 0, and let's set top-k sampling, with k = 1\n",
    "# This class loads the model and in memory.\n",
    "generator = GeneratorManager(model_path, topk = 1)\n",
    "\n",
    "# Now, let's load the tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b45e5c-9a82-46f4-845e-72e34a53d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from uniProt, translated into Finenzyme control codes:  [13, 29, 422, 49, 3]\n",
      "Do we have the taxonomy ID in the tokenizer?  True\n",
      "taxonomy id:  1634\n"
     ]
    }
   ],
   "source": [
    "# An example sequence: P05102\tMTH1_HAEPH\tType II methyltransferase M.HhaI \n",
    "# (source: https://www.uniprot.org/uniprotkb/P05102/entry)\n",
    "sequence = \"MIEIKDKQLTGLRFIDLFAGLGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\"\n",
    "\n",
    "# keyword IDs in input (from UniProt): \n",
    "keywords_uniprot = 'KW-0002; KW-0238; KW-0489; KW-0680; KW-0949; KW-0808'\n",
    "keywords_uniprot = [int(i.split('-')[-1]) for i in keywords_uniprot.split('; ') if i != '']\n",
    "\n",
    "# Convert keywords to control IDs, \n",
    "# given that UniProt goes under continuous updates, we might not have all keywords in the encoding dictionary\n",
    "keywords_finenzyme = [tokenizer.kw_to_ctrl_idx[i] for i in keywords_uniprot if i in tokenizer.kw_to_ctrl_idx.keys()]\n",
    "print('Keywords from uniProt, translated into Finenzyme control codes: ', keywords_finenzyme)\n",
    "\n",
    "# we can also add taxonomy ID as a keyword\n",
    "taxonomy_id = 735 # from UniProt\n",
    "taxonomy_id = tokenizer.taxa_to_ctrl_idx[taxonomy_id] if taxonomy_id in tokenizer.taxa_to_ctrl_idx.keys() else None\n",
    "print('Do we have the taxonomy ID in the tokenizer? ', taxonomy_id is not None)\n",
    "print('taxonomy id: ', taxonomy_id)\n",
    "\n",
    "# next, we add the taxonomy Finenzyme tag into the list of tags in input to the model\n",
    "keywords_finenzyme.append(taxonomy_id)\n",
    "\n",
    "# Next, we set the amino acid prefix to give in input to the model\n",
    "prefix = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "588115bb-787a-4eca-8c79-9b3331e0a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: generation. With after_n_generation we generate up to the real protein length.\n",
    "res, tokens_prob, offset = generator.after_n_generation(sequence, keywords_finenzyme, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f007c84-db45-4c8e-85e0-845e16463a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGen generated a protein with a prefix of  20 amino acids.\n",
      "Is the predicted protein equal to the real one? False\n",
      "Generated protein:\n",
      "IGGFHAALHRLGGRCVYASEIDPHVRKVYELNFGDRPEFTEDIRKITEEEIPDHDVLLAGFPCQPFSIIGKRRGFKDERGTLYFEILRILKAKRPRAFLLENVKNFVNHDKGRTFKIIEDVLEELDFSFSYKLLDPKNFGVPQNRERVFIVGFREKEKLDFKFPKPEELPKPLTLSDILEDNPDSQYFLSKDKLTKLHRHKEKGNGFGFGLVNIEGKGGKIARTLSARYHKDNIDNIEGARNNARPNHQANGIPLSPQQAAKIQGFPEDFKIIGNDAVYKQLGNAVVVPLIQAIGEKILKELNKEKK\n",
      "Actual protein:\n",
      "LGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\n"
     ]
    }
   ],
   "source": [
    "# What happened during generation?\n",
    "print('Finenzyme generated a protein with a prefix of ', offset, 'amino acids.')\n",
    "print('Is the predicted protein equal to the real one?', res == sequence[prefix:])\n",
    "print('Generated protein:')\n",
    "print(res)\n",
    "print('Actual protein:')\n",
    "print(sequence[prefix:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57794df1-2d32-4113-af53-9f1cb9f7b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained model generated an enzyme with 259 different amino acids from the original one.\n"
     ]
    }
   ],
   "source": [
    "# here we search for indices of amino acids that differ from the natural sequence\n",
    "differing_idx = [(i, [actual, predicted]) for i, (actual, predicted) in enumerate(zip(sequence[prefix:], res)) if actual != predicted]\n",
    "print(f'The pre-trained model generated an enzyme with {len(differing_idx)} different amino acids from the original one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b25b64b6-0904-49b0-899a-ff0eb3df271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sequence in generated index 0, has probabilities for true aa L of 0.169, and for predicted aa I of 0.666\n"
     ]
    }
   ],
   "source": [
    "# we can analyze the probabilities of these indices that we have in output from Finenzyme\n",
    "from tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "for difference in differing_idx:\n",
    "    print(f'The predicted sequence in generated index {difference[0]},'\n",
    "    f' has probabilities for true aa {difference[1][0]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][0]]]:.3f},'\n",
    "    f' and for predicted aa {difference[1][1]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][1]]]:.3f}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f400fbd-f071-40e0-9238-8c8b34ee1e5a",
   "metadata": {},
   "source": [
    "### Comment on the Generation with the Pre-trained Model\n",
    "\n",
    "In this section, we discuss the results obtained from generating protein sequences using the pre-trained model. The model was guided by specific UniProt keywords and a taxonomy ID to produce sequences relevant to the desired biological context.\n",
    "\n",
    "#### Results Analysis\n",
    "The generation process began with the input sequence of the Type II methyltransferase P05102 and relevant UniProt keywords related to this entry, translated into control codes for the model. A taxonomy ID was also added to refine the context further.\n",
    "\n",
    "The model generated a sequence with a specified prefix length, and the resulting sequence was compared to the original protein sequence. Key findings:\n",
    "\n",
    "- **Generated Sequence**: The pre-trained model produced a protein sequence with an initial prefix of 20 amino acids provided as input. The rest of the sequence was generated based on the model's learned patterns and the specified keywords.\n",
    "- **Comparison to Actual Sequence**: The generated sequence was compared to the actual sequence starting from the 20th amino acid. It was noted that there were differences in several amino acids between the generated and actual sequences.\n",
    "- **Differing Indices**: The specific indices where the generated sequence differed from the actual sequence were identified. The analysis found a certain number of amino acids that did not match, indicating areas where the model's predictions deviated from the natural sequence.\n",
    "- **Probability Analysis**: For the differing amino acids, the probabilities assigned by the model to both the actual and predicted amino acids were examined. This provides insight into the model's confidence in its predictions and highlights the areas where it may have uncertain.\n",
    "\n",
    "#### Conclusion\n",
    "The pre-trained model generated a protein sequence influenced by the input keywords and taxonomy ID. Even with the use of strict sampling methods (top-k with k = 1), the generated sequence was not identical to the natural one, the differences provide valuable information about the model's predictive capabilities for further improvement (i.e. fine-tuning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cfa88-005a-465f-b639-b033a214286d",
   "metadata": {},
   "source": [
    "<h2>Step 2. Fine-tuning the model on a specific protein set</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d6f61-e61c-4b17-bcb3-21cb1817b4e4",
   "metadata": {},
   "source": [
    "<p>Dataset preparation: descrivi</p>\n",
    "pickle dizionario con [kws definite e già codificate, amino acidi (codificati da lettere)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecee2cd-abd3-4fe9-a6fd-691f9b9b26b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78e9d206-7250-41a3-896e-033f49dd1810",
   "metadata": {},
   "source": [
    "<p>Il finetuning di Finenzyme è svolto dal modulo pytorch_training.py, basta chiamarlo direttamente da linea di comando dando in input: il modello pre-trained di partenza, il dataset di training, opzionalmente il dataset di validation, e tutti i possibili parametri del fine-tuning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224bc8e-8754-432d-8a38-2946e41f8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCLAIMER: this step requires a huge amount of time, parallelized hardware (GPU) is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acb223-7f97-4352-90f6-662d69df99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command as a string\n",
    "command = \"\"\"\n",
    "python pytorch_training.py --model_dir 'ckpt/' --model_path 'ckpt/pretrain_progen_full.pth' --stop_token 1 --model_name 'ec_2_1_1_37' --db_directory 'data_specific_enzymes/databases/pickles/'\n",
    "\"\"\"\n",
    "\n",
    "# Use subprocess to run the command\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Print the output line by line as it's being generated\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    print(line.decode(), end='')\n",
    "\n",
    "# Print the error messages, if any\n",
    "for line in iter(process.stderr.readline, b''):\n",
    "    print(line.decode(), end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9b610-79c1-495b-a613-ed7921b15b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comandi da chiamare dal notebook.....\n",
    "# e spiegazione......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd404ea-393f-4f85-866e-2973a4788765",
   "metadata": {},
   "source": [
    "<h2>Step 3. Generation through the fine-tuned model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76061741-2cd8-4a99-8921-78333e23b551",
   "metadata": {},
   "source": [
    "<p>Paragrafo test</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b3f274-7635-4e28-974c-963116e70fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_manager import GeneratorManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6828d655-0275-414a-8b7d-540e42c26462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SIZE: \n",
      "1280\n",
      "Found PyTorch checkpoint at  ckpt/ec_2_1_1_37_warmup_1000_earlystop_epoch6_015_flip_LR01_2batch.pth\n",
      "GPU aviable. Previous checkpoint loaded in GPU\n"
     ]
    }
   ],
   "source": [
    "# Here we set the model checkpoint path, \n",
    "# for this example, we use Finenzyme on phage lysozymes.\n",
    "model_path = 'ckpt/ec_2_1_1_37_warmup_1000_earlystop_epoch6_015_flip_LR01_2batch.pth' \n",
    "\n",
    "# Now, time to set the generator manager with default parameters: penalty = 0, and let's set top-k = 1\n",
    "# This class loads the model and the tokenizer in memory.\n",
    "generator = GeneratorManager(model_path, topk = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26faedec-f4d1-471f-a617-66837d67837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example sequence: P05102\tMTH1_HAEPH\tType II methyltransferase M.HhaI \n",
    "# (source: https://www.uniprot.org/uniprotkb/P05102/entry)\n",
    "sequence = \"MIEIKDKQLTGLRFIDLFAGLGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\"\n",
    "\n",
    "# keyword IDs in input defined during fine-tuning: (already in control code codification) \n",
    "keywords_finenzyme = [0]\n",
    "\n",
    "# Next, we set the amino acid prefix to give in input to the model\n",
    "prefix = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68418986-0abe-48a7-b1f4-648709cff8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: generation. With after_n_generation we generate up to the real protein length.\n",
    "res, tokens_prob, offset = generator.after_n_generation(sequence, keywords_finenzyme, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed03951d-9541-41e5-a1e5-7f7681ea86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGen generated a protein with a prefix of  20 amino acids.\n",
      "Is the predicted protein equal to the real one? False\n",
      "Generated protein:\n",
      "LGGFRLALESFGAECVYSNEWDKYAQEVYQMNFGDKPDGDITLVDENSVPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDVARIVKAKNPKVVFMENVKNFASHDNGNTLKVVKNIMVDLGYDFYSDVLNSLDFGIPQKRERIYMVCFRKDLNIKNFTFPKPFKLSTFLEDLLLPDEEVSNLIINRPDLVLKDIEIKNNSNKTIRIGEVGKGGQGERIYSPKGIAITLSAYGGGVFSKTGGYLINGKTRKLHPRECARIMGYPDSYLIHPSWNQAYKQFGNSVVVNVLQYITKNMGEALSGEYN\n",
      "Actual protein:\n",
      "LGGFRLALESCGAECVYSNEWDKYAQEVYEMNFGEKPEGDITQVNEKTIPDHDILCAGFPCQAFSISGKQKGFEDSRGTLFFDIARIVREKKPKVVFMENVKNFASHDNGNTLEVVKNTMNELDYSFHAKVLNALDYGIPQKRERIYMICFRNDLNIQNFQFPKPFELNTFVKDLLLPDSEVEHLVIDRKDLVMTNQEIEQTTPKTVRLGIVGKGGQGERIYSTRGIAITLSAYGGGIFAKTGGYLVNGKTRKLHPRECARVMGYPDSYKVHPSTSQAYKQFGNSVVINVLQYIAYNIGSSLNFKPY\n"
     ]
    }
   ],
   "source": [
    "# What happened during generation?\n",
    "print('Finenzyme generated a protein with a prefix of ', offset, 'amino acids.')\n",
    "print('Is the predicted protein equal to the real one?', res == sequence[prefix:])\n",
    "print('Generated protein:')\n",
    "print(res)\n",
    "print('Actual protein:')\n",
    "print(sequence[prefix:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd134435-be0b-4921-9bb1-fc99a236da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pre-trained model generated an enzyme with 71 different amino acids from the original one.\n"
     ]
    }
   ],
   "source": [
    "# here we search for indices of amino acids that differ from the natural sequence\n",
    "differing_idx = [(i, [actual, predicted]) for i, (actual, predicted) in enumerate(zip(sequence[prefix:], res)) if actual != predicted]\n",
    "print(f'The pre-trained model generated an enzyme with {len(differing_idx)} different amino acids from the original one.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36d019cb-3ddb-44c2-9831-16fd0bd60279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sequence in index 10, has probabilities for true aa C of 0.017, and for predicted aa F of 0.796\n"
     ]
    }
   ],
   "source": [
    "# we can analyze the probabilities of these indices that we have in output from Finenzyme\n",
    "from tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "for difference in differing_idx:\n",
    "    print(f'The predicted sequence in index {difference[0]},'\n",
    "    f' has probabilities for true aa {difference[1][0]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][0]]]:.3f},'\n",
    "    f' and for predicted aa {difference[1][1]} of {tokens_prob[difference[0]][0][tokenizer.aa_to_probs_index[difference[1][1]]]:.3f}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db7dc-1af7-4e62-8804-8b4fcc526eff",
   "metadata": {},
   "source": [
    "<p>Considerazioni finali</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
