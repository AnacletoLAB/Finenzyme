{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6920e7ba-9e25-4ef6-8f52-1b2bd77032c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "import platform\n",
    "import hashlib\n",
    "import pytorch_transformer\n",
    "import re\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from transformProtein import transformProtein\n",
    "from ProteinDataset_uid import ProteinDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "GPU = torch.cuda.is_available()\n",
    "#if GPU:\n",
    "#    torch.cuda.empty_cache()\n",
    "# if you don't want to use GPU (but you have it) modify in pytorch_transformer.py module the related variable\n",
    "print(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f878f9-08e1-4448-9711-f3786b09174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----vocab size 129407 ------\n"
     ]
    }
   ],
   "source": [
    "load_model_path = 'ckpt/training_ckpt_4/' # just the folder itself  \n",
    "curr_model_path = load_model_path+'model_only_state_dict_v0Last_lr0001.pth'\n",
    "seq_length = 511\n",
    "embedding_dim = 1280\n",
    "num_layers = 36\n",
    "# GENERATION parameters\n",
    "temperature = 0.9\n",
    "penalty = 1.2\n",
    "np.random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "vocab_loc = 'mapping_files/vocab.txt'\n",
    "use_py3 = platform.python_version()[0] == '3'\n",
    "vocab = open(vocab_loc).readlines() if not use_py3 else open(vocab_loc, encoding='utf-8').read().split('\\n')[:-1]\n",
    "vocab = list(map(lambda x: x.split(' ')[0], vocab))\n",
    "vocab_size = len(vocab)\n",
    "print('-----vocab size',vocab_size,'------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a359416-c643-4c75-8efc-296f411ca5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SIZE: \n",
      "1280\n",
      "model initialized\n",
      "loading model from:  ckpt/training_ckpt_4/model_only_state_dict_v0Last_lr0001.pth\n",
      "Found PyTorch checkpoint at  ckpt/training_ckpt_4/model_only_state_dict_v0Last_lr0001.pth\n",
      "Loading checkpoint...\n",
      "previous checkpoint loaded\n",
      "previous checkpoint loaded in GPU\n"
     ]
    }
   ],
   "source": [
    "class TiedEmbeddingSoftmax(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, embedding_size=embedding_dim, **kwargs):\n",
    "        super(TiedEmbeddingSoftmax, self).__init__()\n",
    "        self.w = torch.nn.Parameter(torch.normal(0., 1e-2, size=(vocab_size, embedding_size)))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(vocab_size))\n",
    "\n",
    "    def forward(self, inputs, embed=True):\n",
    "        if embed:\n",
    "            return torch.nn.functional.embedding(inputs, self.w)\n",
    "        else:\n",
    "            return torch.tensordot(inputs, self.w.t(), 1) + self.b\n",
    "\n",
    "class CTRLmodel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CTRLmodel,self).__init__()\n",
    "        self.tied_embedding_softmax = TiedEmbeddingSoftmax()\n",
    "        self.encoder = pytorch_transformer.Encoder()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.tied_embedding_softmax(inputs, embed=True)\n",
    "        x = self.encoder(x)\n",
    "        x = self.tied_embedding_softmax(x, embed=False)\n",
    "        return x\n",
    "\n",
    "    def loadCheckpoint(self, model_path, num_layers):\n",
    "        if os.path.exists(model_path):\n",
    "            print('Found PyTorch checkpoint at ', model_path)\n",
    "            print('Loading checkpoint...')\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            if 'epoch' in checkpoint.keys():\n",
    "                print(checkpoint.keys())\n",
    "                checkpoint = checkpoint['model_state_dict']\n",
    "            self.tied_embedding_softmax.load_state_dict({\n",
    "                'w': checkpoint.pop('tied_embedding_softmax.w', None),\n",
    "                'b': checkpoint.pop('tied_embedding_softmax.b', None)\n",
    "            })\n",
    "            self.encoder.load_state_dict({key.replace(\"encoder.\", \"\"): value for key, value in checkpoint.items()})\n",
    "        else:\n",
    "            print('Could not find PyTorch checkpoint')\n",
    "            sys.exit()\n",
    "\n",
    "model = CTRLmodel()\n",
    "print('model initialized')\n",
    "print('loading model from: ', curr_model_path)\n",
    "reader = model.loadCheckpoint(model_path=curr_model_path, num_layers = num_layers)\n",
    "print('previous checkpoint loaded')\n",
    "if GPU:\n",
    "    model = model.cuda()\n",
    "    print('previous checkpoint loaded in GPU')\n",
    "optimizer = torch.optim.Adam(model.parameters()) #lr, betas\n",
    "model.eval()\n",
    "\n",
    "with open(os.path.join('mapping_files/','taxa_to_lineage.p'),'rb') as handle:\n",
    "    taxa_to_lineage = pickle.load(handle)\n",
    "with open('mapping_files/taxa_to_ctrl_idx.p','rb') as handle:\n",
    "    taxa_to_ctrl_idx = pickle.load(handle)\n",
    "with open('mapping_files/kw_to_ctrl_idx.p','rb') as handle:\n",
    "    kw_to_ctrl_idx = pickle.load(handle)\n",
    "with open('mapping_files/aa_to_ctrl_idx.p','rb') as handle:\n",
    "    aa_to_ctrl_idx = pickle.load(handle)\n",
    "with open('mapping_files/kw_to_name.p2','rb') as handle:\n",
    "    kw_to_name = pickle.load(handle)\n",
    "    \n",
    "def flipdict(my_map):\n",
    "    return {v: k for k, v in my_map.items()}\n",
    "ctrl_idx_to_aa = flipdict(aa_to_ctrl_idx)\n",
    "ctrl_idx_to_kw = flipdict(kw_to_ctrl_idx)\n",
    "ctrl_idx_to_taxa = flipdict(taxa_to_ctrl_idx)\n",
    "\n",
    "def predict_fn(inputs):\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(inputs)\n",
    "        if GPU:\n",
    "            inputs = inputs.cuda()            \n",
    "        output = model(inputs)\n",
    "        stop_token = output[:, :, 1] # the stop token logits\n",
    "        output = output[:,:,-26:-1] # remove non-AA token logits\n",
    "        return output, stop_token\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52338a0f-9f55-41ee-a8ea-95dd3eb10719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequences(predicted, predicted_stopped, molechule_name_and_params):\n",
    "    data_dir = \"Generation_PF00959_model_lr_00001_phage_specific\"\n",
    "    query = molechule_name_and_params\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    predicted_data_file = os.path.join(data_dir, \"predicted_data_\" + query + \".p\")\n",
    "    with open(predicted_data_file, \"wb\") as file:\n",
    "        pickle.dump(predicted, file)\n",
    "\n",
    "    predicted_stopped_data_file = os.path.join(data_dir, \"predicted_stopped_data_\" + query + \".p\")\n",
    "    with open(predicted_stopped_data_file, \"wb\") as file:\n",
    "        pickle.dump(predicted_stopped, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfe7048-a976-4c86-9a0c-9f7e1d5716f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_complete_sequence(input_sequence, after_n, tax_lineage, top_p):\n",
    "    res = \"\"\n",
    "    res_stopped = []\n",
    "    # tokens_prob = []\n",
    "    key_len = len(tax_lineage) # len(kw_lineage+tax_lineage)\n",
    "    i = after_n # if we have a sequence of amminoacids in input, we can add some in input as seed sequence\n",
    "    iteration_input_prefix = input_sequence[:i]\n",
    "    seed_seq = [aa_to_ctrl_idx[ii] for ii in iteration_input_prefix]\n",
    "    generate_num = 511\n",
    "    seq_length = min(generate_num, 511)\n",
    "    text = tax_lineage + seed_seq # tax_lineage + kw_lineage + seed_seq\n",
    "    padded_text = text + [0] * (generate_num - len(text))\n",
    "    tokens_generated = np.tile(padded_text, (1,1))\n",
    "    for token in range(len(text)-1, generate_num-1):\n",
    "        # prediction\n",
    "        prompt_logits, stop_token = predict_fn(tokens_generated[:, :seq_length])\n",
    "        prompt_logits = prompt_logits.squeeze()  / (temperature if temperature>0 else 1.)\n",
    "        stop_token = stop_token.squeeze()  / (temperature if temperature>0 else 1.)\n",
    "        _token = token if token < seq_length else -1\n",
    "        prompt_logits = prompt_logits.cpu().detach().numpy()\n",
    "        stop_token = stop_token.cpu().detach().numpy()\n",
    "        # penalty\n",
    "        if (penalty>0) and (token >= key_len + 3):\n",
    "            penalized_so_far = set()\n",
    "            for _ in range(token-3,token+1):\n",
    "                generated_token = tokens_generated[0][_] - (vocab_size-26) # added\n",
    "                if generated_token in penalized_so_far:\n",
    "                    continue\n",
    "                penalized_so_far.add(generated_token)\n",
    "                prompt_logits[_token][generated_token] /= penalty  \n",
    "        # compute probabilities from logits\n",
    "        prompt_probs = np.exp(prompt_logits[_token])\n",
    "        prompt_probs = prompt_probs / sum(prompt_probs)\n",
    "        # ESTRARRE TOKEN 1: the stop token, softmax con le probabilità degli amminoacidi\n",
    "        logits_and_stop = np.concatenate((prompt_logits[_token], [stop_token[_token]]))\n",
    "        logits_and_stop_prob = np.exp(logits_and_stop)\n",
    "        logits_and_stop_prob = logits_and_stop_prob / sum(logits_and_stop_prob)\n",
    "        if logits_and_stop_prob[-1] >= top_p:\n",
    "            tokens_generated_stopped = tokens_generated[0][len(seed_seq) + key_len:_token + 1]\n",
    "            tokens_generated_stopped = ''.join([ctrl_idx_to_aa[c] for c in tokens_generated_stopped])\n",
    "            res_stopped.append(tokens_generated_stopped)\n",
    "        pruned_list = np.argsort(prompt_probs)[::-1]\n",
    "        # tokens_prob.append([prompt_probs.tolist()])\n",
    "        if top_p==1:\n",
    "            idx = pruned_list[0]\n",
    "        else:\n",
    "            # Sort the probabilities\n",
    "            sorted_probs, sorted_indices = torch.sort(torch.tensor(prompt_probs), descending=True)\n",
    "            # Calculate cumulative probabilities\n",
    "            cum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "            # Get the set of tokens whose cumulative probability is less than or equal to p (e.g., 0.9)\n",
    "            valid_indices = sorted_indices[cum_probs <= top_p]\n",
    "            # If no token's cumulative probability is less than the threshold, just select the top token\n",
    "            if valid_indices.size(0) == 0:\n",
    "                valid_indices = sorted_indices[:1]\n",
    "            # Sample from the valid indices\n",
    "            idx = valid_indices[torch.randint(0, valid_indices.size(0), (1,))].item()\n",
    "        # assign the token for generation\n",
    "        idx += (vocab_size-26) # added to convert 0 AA to original ctrl idx\n",
    "        tokens_generated[0][token+1] = idx\n",
    "    tokens_generated = tokens_generated[0][len(seed_seq) + key_len:]\n",
    "    tokens_generated = ''.join([ctrl_idx_to_aa[c] for c in tokens_generated])\n",
    "    return tokens_generated, res_stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3597988-128d-4cb7-906b-f25bf5066413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save(top_p, seq_number_to_generate, protein_base, protein, protein_name):\n",
    "    predicted = []\n",
    "    predicted_stopped = []\n",
    "    seq_number = 0\n",
    "    input_seq = protein\n",
    "    tax_lineage = [0]\n",
    "    offset = int(protein_base * len(input_seq))\n",
    "    # Adjust for zero-based indexing\n",
    "    offset = offset - 1 if offset > 0 else 0\n",
    "    #print(offset)\n",
    "    print('generating sequences...')\n",
    "    while seq_number < seq_number_to_generate:\n",
    "        #start_time = time.time()\n",
    "        res, tokens_generated_stopped = generation_complete_sequence(input_seq, offset, tax_lineage, top_p)\n",
    "        #end_time = time.time()\n",
    "        #elapsed_time = end_time - start_time\n",
    "        #print(f\"Done. Time taken: {elapsed_time} seconds.\")\n",
    "        predicted.append(input_seq[:offset] + res)\n",
    "        if tokens_generated_stopped:\n",
    "            for tmp_seq in tokens_generated_stopped:\n",
    "                predicted_stopped.append(input_seq[:offset] + tmp_seq)\n",
    "        seq_number += 1\n",
    "        # print('hello, this is seq_number', seq_number)\n",
    "        if ((seq_number%10) == 0) or (seq_number == (seq_number_to_generate-1)):\n",
    "            # print('the model has generated: ', seq_number , ' sequences')\n",
    "            print('saving sequences')\n",
    "            description = protein_name + '_top_p_' + str(top_p) + 'seed_percentage_' + str(protein_base)\n",
    "            save_sequences(predicted, predicted_stopped, description)\n",
    "    print('GENERATION BATCH ENDED')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb70ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file with a single protein sequence.\n",
    "    Args:\n",
    "    file_path (str): The path to the FASTA file.\n",
    "    Returns:\n",
    "    tuple: A tuple containing the identifier and the protein sequence as strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        identifier = lines[0].strip()[4:21]  # Remove '>' and any trailing newline character\n",
    "        sequence = ''.join(line.strip() for line in lines[1:])  # Concatenate the remaining lines\n",
    "        return identifier, sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_and_identifiers = []\n",
    "fasta_dir = 'FASTA_lysozymes_of_interest/'\n",
    "for file in os.listdir(fasta_dir):\n",
    "    sequences_and_identifiers.append((read_fasta(fasta_dir + file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9eca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q37875|ENLYS_BPP1\n",
      "185\n",
      "MKGKTAAGGGAICAIAVMITIVMGNGNVRTNQAGLELIGNAEGCRRDPYMCPAGVWTDGIGNTHGVTPGVRKTDQQIAADWEKNILIAERCINQHFRGKDMPDNAFSAMTSAAFNMGCNSLRTYYSKARGMRVETSIHKWAQKGEWVNMCNHLPDFVNSNGVPLRGLKIRREKERQLCLTGLVNE\n",
      "P03706|ENLYS_LAMB\n",
      "158\n",
      "MVEINNQRKAFLDMLAWSEGTDNGRQKTRNHGYDVIVGGELFTDYSDHPRKLVTLNPKLKSTGAGRYQLLSRWWDAYRKQLGLKDFSPKSQDAVALQQIKERGALPMIDRGDIRQAIDRCSNIWASLPGAGYGQFEHKADSLIAKFKEAGGTVREIDV\n",
      "P78285|LYSD_ECOLI\n",
      "165\n",
      "MPPSLRKAVAAAIGGGAIAIASVLITGPSGNDGLEGVSYIPYKDIVGVWTVCHGHTGKDIMLGKTYTKAECKALLNKDLATVARQINPYIKVDIPETTRGALYSFVYNVGAGNFRTSTLLRKINQGDIKGACDQLRRWTYAGGKQWKGLMTRREIEREVCLWGQQ\n",
      "P00720|ENLYS_BPT4\n",
      "164\n",
      "MNIFEMLRIDERLRLKIYKDTEGYYTIGIGHLLTKSPSLNAAKSELDKAIGRNCNGVITKDEAEKLFNQDVDAAVRGILRNAKLKPVYDSLDAVRRCALINMVFQMGETGVAGFTNSLRMLQQKRWDEAAVNLAKSIWYNQTPNRAKRVITTFRTGTWDAYKNL\n"
     ]
    }
   ],
   "source": [
    "for i, seq in sequences_and_identifiers:\n",
    "    print(i)\n",
    "    print(len(seq))\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffdeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sequences...\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "GENERATION BATCH ENDED\n",
      "0.25 150 0.25 MKGKTAAGGGAICAIAVMITIVMGNGNVRTNQAGLELIGNAEGCRRDPYMCPAGVWTDGIGNTHGVTPGVRKTDQQIAADWEKNILIAERCINQHFRGKDMPDNAFSAMTSAAFNMGCNSLRTYYSKARGMRVETSIHKWAQKGEWVNMCNHLPDFVNSNGVPLRGLKIRREKERQLCLTGLVNE Q37875|ENLYS_BPP1\n",
      "------\n",
      "generating sequences...\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n",
      "saving sequences\n"
     ]
    }
   ],
   "source": [
    "top_ps = [0.25, 0.5, 0.75]\n",
    "after_percentage = [0.25, 0.5, 0.75]\n",
    "number_of_sequences_to_generate = 150 # 1 è un test: poi 200\n",
    "for protein_name, protein in sequences_and_identifiers:\n",
    "    for after_p in after_percentage:\n",
    "        for p in top_ps:\n",
    "            generate_and_save(p, number_of_sequences_to_generate, after_p, protein, protein_name)\n",
    "            print(p, number_of_sequences_to_generate, after_p, protein, protein_name)\n",
    "            print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e173c-14b4-4503-acd3-bc9d8e5ca1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
